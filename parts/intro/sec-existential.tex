\section{Existentialism is a Database Theory}
% Existentialism is a Humanism
% - Jean-Paul Sartre
\label{sec:intro-existential}

\begin{marginfigure}[15em]
	\centering
	\includegraphics[width=\linewidth]{fig/intro/aliceroom.jpg}
	\caption{\href{https://commons.wikimedia.org/wiki/File:Aliceroom.jpg}{\emph{Looking glass room}}, by John Tenniel.}
\end{marginfigure}

\subsection{Conjunctive Queries}
\label{sec:intro-cq}

We now turn to the $\textsf{query} \homto^? \textsf{data}$ side of the homomorphism problem. This perspective captures classical database evaluation and highlights how such queries naturally express existential, monotonic properties.
For instance, if $\?G$ is the "graph@@dir" with
two nodes $u$ and $v$ and a single edge from $u$ to $v$,
then asking if there is a "homomorphism" from $\?G$ to a graph $\?H$ amounts
to asking if there exists at least one edge in $\?H$.

As we would expect for any existential problem, they are monotonic:
if a solution exists, and we add more data, then a solution still exists.
More formally, for any "structure" $\?A$, $\?B$ and $\?B'$, if $\?B$ is a "substructure"
of $\?B'$ and $\?A \homto \?B$,
then $\?A \homto \?B'$.

SQL queries (\Cref{ex:sql-as-hom}) actually represent more than a mere example:
every "homomorphism problem" $\?A \homto \?B$ can be seen as a SQL query evaluation problem
where $\?A$ encodes a "query@@sem" in the \textsf{SELECT-FROM-WHERE}
fragment of SQL and $\?B$ is encodes a relational database.
This fragment can also be characterized as the fragment
of "first-order logic" where neither universal quantification, nor negation nor union is allowed.
For instance, the SQL query of \Cref{ex:sql-as-hom} can be expressed by the formula
\begin{align*}
	\phi(\textsf{title}, \textsf{time}) \defeq\; 
	& \exists \textsf{movie\_id}.\, 
	\exists \textsf{length}.\, 
	\exists \textsf{director}.\,
	\exists \textsf{room\_id}.\, \\
	& \hphantom{\land~} \textsc{Movies}(\textsf{movie\_id}, \textsf{title}, \textsf{length}, \textsf{director}) \\
	& \land
	\textsc{Projections}(\textsf{movie\_id}, \textsf{room\_id}, \textsf{time}).
\end{align*} 
\begin{marginfigure}
	\centering
	\begin{tikzpicture}
		\node (a0) at (-.1,5.2) {};
		\node (a1) at (-.4,3.5) {};
		\node (a2) at (-.1,1.8) {};
		\node (a3) at (.1,1.8) {};
		\node (a4) at (.4,3.5) {};
		\node (a5) at (.1,5.2) {};

		\node (b0) at (-.2,5.2) {};
		\node (b1) at (-.8,3.1) {};
		\node (b2) at (-.5,1.5) {};
		\node (b3) at (-.4,.6) {};
		\node (b4) at (-.2,-.2) {};
		\node (b5) at (.2,-.2) {};
		\node (b6) at (.4,1) {};
		\node (b7) at (.2,1.4) {};
		\node (b8) at (-.5,2.6) {};
		\node (b9) at (-.4,3.8) {};
		\node (b10) at (.2,5.2) {};
	
		\draw[use Hobby shortcut, closed=true, draw=c2, fill=c2, fill opacity=.4] 
			(b0) .. (b1) .. (b2) .. (b3) .. (b4) .. (b5) .. (b6) .. (b7) .. (b8) .. (b9) .. (b10);
		\draw[use Hobby shortcut, closed=true, draw=c1, fill=c1, fill opacity=.4] 
			(a0) .. (a1) .. (a2) .. (a3) .. (a4) .. (a5);
		
		\node[vertex] at (0,5) (5) {};
		\node[vertex] at (0,4) (4) {};
		\node[vertex] at (0,3) (3) {};
		\node[vertex] at (0,2) (2) {};
		\node[vertex] at (0,1) (1) {};
		\node[vertex] at (0,0) (0) {};
		\node[font=\tiny, right=2em of 5] {\textsf{movie\_id}};
		\node[font=\tiny, right=2em of 4] {\textsf{title}};
		\node[font=\tiny, right=2em of 3] {\textsf{length}};
		\node[font=\tiny, right=2em of 2] {\textsf{director}};
		\node[font=\tiny, right=2em of 1] {\textsf{room\_id}};
		\node[font=\tiny, right=2em of 0] {\textsf{time}};

		\node[below= of 0] {output: \textsf{title}, \textsf{time}};
	\end{tikzpicture}
	\caption{
		\AP\label{fig:SQL-as-CQ}
		A "conjunctive query".
	}
\end{marginfigure}
Yet another characterization of these queries as "conjunctive queries",
that consist of a "relational structure" together with a tuple of vertices, called ``"output@@var"''---this tuple
plays the same role as the \textsf{SELECT} statement in SQL.
For instance, the previous query can be expressed as the "conjunctive query"
of \Cref{fig:SQL-as-CQ}. The semantics of such a "query@CQ"
$\gamma = \tup{\?G, \bar x}$ is defined as follows:
given a relational database, seen as a "relational structure" $\?D$,
it returns every possible tuple $\bar d$ of elements of $\?D$ such that
there exists a "homomorphism" from $\?G$ to $\?D$ that sends $\bar x$ to $\bar d$.
Notice how this formalism uses the connection with "homomorphisms"
described in \Cref{ex:sql-as-hom} to define a semantics.
Overall, these characterizations show this query language to be quite robust.

\begin{known}
	Problems of the form $\textsf{query} \homto^? \textsf{data}$
	exactly correspond to the evaluation of "conjunctive query". 
	Finite unions of "conjunctive queries" can also be described as the
	\textsf{SELECT-FROM-WHERE} fragment of SQL, or as the "existential-positive
	fragment" of "first-order logic".
\end{known}

We now turn to the complexity of evaluating these "queries@CQ".
As we have seen, "conjunctive query evaluation" boils down to
"homomorphism problems" of the form $\textsf{query} \homto^? \textsf{data}$.
Assuming that the "query@@sem" if fixed,
the naive algorithm to solve the "homomorphism problem"---consisting in enumerating
every possible function from $A$ to $B$ and checking whether some of them
define a "homomorphism"---works in polynomial time, as there are only $|B|^{|A|}$ such functions.
In fact, it is straightforward to devise an algorithm in "uniform-AC0"---actually, the depth
of the circuit does not even depend on $\?A$: there is roughly one layer simulating the
existential quantifiers, and another one simulating the conjunctions.

While $|B|^{|A|}$ is indeed polynomial when $\?A$ is fixed,
recall that $\?B$ represents a database:
even though most theoreticians pretend that polynomial-time is tractable,
a polynomial-time algorithm of degree seven,
run over a 2.9 TB database, will execute more instructions
than there are atoms in the observable universe.
This leads to two natural directions:
\begin{itemize}
	\item optimizing the size of the exponent, "ie"
		replacing the "query@@sem" with a semantically equivalent one of smaller size,
	\item studying the parametrized complexity of evaluating SQL queries, when parametrized by
		the size of the query. This provides a finer complexity notion than
		the classical "NP"/"AC0" approach; our previous remark shows the result to be
		slicewise polynomial ("XP"), which is not as well-behaved in practice as
		"fixed-parameter tractable" ("FPT") problems.
\end{itemize}

\begin{figure}
	\centering
	\begin{tikzpicture}
		\node[vertex, draw=c1, fill=c1, fill opacity=.4] at (0,1) (a) {};
		\node[vertex, draw=c0, fill=c0, fill opacity=.4] at (1.414,1) (b) {};
		\node[vertex, draw=c2, fill=c2, fill opacity=.4] at (2.828,1) (c) {};
		\node[vertex, draw=c1, fill=c1, fill opacity=.4] at (4.242,1) (d) {};
		\node[vertex, draw=c2, fill=c2, fill opacity=.4] at (0.707,0) (e) {};
		\node[vertex, draw=c1, fill=c1, fill opacity=.4] at (2.121,0) (f) {};
		\node[vertex, draw=c3, fill=c3, fill opacity=.4] at (3.536,0) (g) {};
		\node[vertex, draw=c0, fill=c0, fill opacity=.4] at (4.950,0) (h) {};

		\draw[edge] (a) to (b);
		\draw[edge] (b) to (e);
		\draw[edge] (f) to (b);
		\draw[edge] (b) to (c);
		\draw[edge] (f) to (c);
		\draw[edge] (g) to (f);
		\draw[edge] (c) to (g);
		\draw[edge] (d) to (c);
		\draw[edge] (g) to (d);
		\draw[edge] (d) to (h);

		\begin{scope}[xshift=5.5cm]
			\node[vertex, draw=c0, fill=c0, fill opacity=.4] at (1.414,1) (b2) {};
			\node[vertex, draw=c2, fill=c2, fill opacity=.4] at (2.828,1) (c2) {};
			\node[vertex, draw=c1, fill=c1, fill opacity=.4] at (2.121,0) (f2) {};
			\node[vertex, draw=c3, fill=c3, fill opacity=.4] at (3.536,0) (g2) {};

			\draw[edge] (f2) to (b2);
			\draw[edge] (b2) to (c2);
			\draw[edge] (f2) to (c2);
			\draw[edge] (g2) to (f2);
			\draw[edge] (c2) to (g2);
		\end{scope}
	\end{tikzpicture}
	\caption{
		\AP\label{fig:intro-core}
		On the left-hand side a "graph@@dir", and its "core" on the right.
		The colours are not part of the "structure", but
		are used to describe a "homomorphism" from the original
		structure to its "core".
	}
\end{figure}
\paragraph*{Query minimization.}
The problem of optimizing the \textsf{SELECT-FROM-WHERE} fragment of
SQL is well-understood, precisely by using the framework of "conjunctive queries"
and "relational structures".
This problem amounts to, given a "finite $\sigma$-structure" $\?A$,
deciding if there exists a strictly smaller "$\sigma$-structure" $\?A'$ "st",%
\footnote{Here, our size measure is simply the number of vertices of the structure.}
for any "finite $\sigma$-structure" $\?B$, then
\[
	\?A \homto \?B
	\quad\text{"iff"}\quad
	\?A' \homto \?B.
\]
The property above is in fact equivalent to
\begin{equation}
	\?A \homto \?A'
	\quad\text{and}\quad
	\?A' \homto \?A
	\label{eq:intro-hom-equivalence}
\end{equation}
and is hence decidable.
The optimization procedure then goes as follows:
starting from $\?A$, we check for every possible vertex $a \in A$
if $\?A \smallsetminus \{a\}$ is equivalent to $\?A$ in the sense of
\Cref{eq:intro-hom-equivalence}. If some $a$ satisfy the property, we
let $\?A \smallsetminus \{a\}$ be our new query and start the process again.
Otherwise, we get a minimal "query@@CQ", called "core" of $\?A$.
This "core" is unique---which is not obvious since we defined it with
a greedy procedure---and is, by construction, a "substructure" of $\?A$,
see \Cref{fig:intro-core}.
In particular, it implies that the "core" does not only minimize the number of
vertices of $\?A$ while being "semantically equivalent": it also minimizes any
parameter that is closed under taking "substructures", such as "eg"
the number of edges or the "tree-width"!
Therefore, this notion of "core", together with seeing
\textsf{SELECT-FROM-WHERE} queries as "relational structures"/"conjunctive queries"
provides a remarkably robust tool for solving most optimization problem on these "queries@@CQ".

\begin{known}
	"Conjunctive queries" can be minimized by computing their "core".
	This process minimizes the number of variables/vertices, but also many other
	parameters, such as the number of edges, the "path-width", the "tree-width", etc.
\end{known}

\paragraph*{FPT algorithms.}
In short, the field of parametrized complexity
studies the computational complexity of decision problems
in a finer way than classical complexity theory: each problem
is associated to a parameter---which is smaller than the size of the whole instance---and
the goal is to understand the influence of the size of this parameter on the
complexity of the problem.
Let us mention the parametrized classes "FPT" and "W[1]", which are roughly
the parametrized equivalent of "P" and "NP".
The problem of whether a "graph@@undir" contains a "$k$-clique",
when parametrized by $k$, is known to be "W[1]"-complete:
in some sense, it means that this problem is hard, and that
this parameter $k$ plays a crucial role in the hardness of the problem.

The problem of whether a "graph@@undir" contains a "$k$-clique" easily reduces to a
"homomorphism problem" where the "source structure" is fixed---and equal to the "$k$-clique".
It follows that the "homomorphism problem", parametrized in the
size of the "source structure" is "W1"-hard. Unfortunately, assuming that "W1" $\neq$ "FPT",%
\footnote{This is the parametrized counterpart of "P" $\neq$ "NP".}
it follows that there cannot be an "FPT" ("ie" ``efficient'')
algorithm for evaluating "conjunctive queries".
Hence, a quest emerged to find classes of "conjunctive queries" with an "FPT" evaluation.

\begin{marginfigure}
	\centering
	\begin{tikzpicture}
		\node[vertex] at (0,0) (eps) {};
		\node[vertex, below left=1.6em and 2em of eps] (a) {};
		\node[vertex, below right=1.6em and 2em of eps] (b) {};
		\node[vertex, below left=1.6em and 1.5em of a] (aa) {};
		\node[vertex, below=1.44em of a] (ab) {};
		\node[vertex, below right=1.6em and 1.5em of a] (ac) {};
		\node[vertex, below=1.44em of b] (ba) {};

		\draw[edge] (eps) to node[above left] {$\+R$} (a);
		\draw[edge] (eps) to node[above right] {$\+S$} (b);
		\draw[edge] (a) to node[above left] {$\+R$} (aa);
		\draw[edge] (a) to node[left] {$\+S$} (ab);
		\draw[edge] (a) to node[above right] {$\+T$} (ac);
		\draw[edge] (b) to node[right] {$\+R$} (ba);
	\end{tikzpicture}
	\caption{
		\AP\label{fig:intro-tree-shaped-CQ}
		A "tree-shaped" "conjunctive query" over a "signature"
		with three binary relations denoted by $a$, $b$ and~$c$.
	}
\end{marginfigure}
First, one can notice that if said query is "tree-shaped",
such as the "conjunctive query" of \Cref{fig:intro-tree-shaped-CQ}, then the naive
bottom-up evaluation algorithm works in time that is polynomial both
in the size of the query and in the size of the database.
Now, assume that $\gamma$ is a "conjunctive query" that is not
tree-shaped, but that is equivalent to a tree-shaped "query@@CQ".
This is the same as saying that the "core" of $\gamma$ is "tree-shaped".
Hence, to evaluate $\gamma$, one can instead:
\begin{itemize}
	\item first compute its "core",%
	\footnote{To quote K-Maro: ``Donne-moi ton coeur, baby, ton core, baby...''}
	\item then evaluate this "core" on the database.
\end{itemize}
The interest of this approach is that, while databases are big and ever-changing,
queries are short and fixed. Hence, spending effort optimizing them can be beneficial,
since it might lead to performance gain
for \emph{every} evaluation of the query: this is why studying the
complexity of the evaluation problem parametrized by the size of
the query is relevant.
Formally, the previous procedure yields an algorithm that works in time
\[
	\+O(f(|\text{query}|) \cdot \poly(|\text{core}|, |\text{database}|)).
\]
This precisely means that evaluating "conjunctive queries" that are semantically equivalent to
tree-shaped "queries@CQ" is "FPT".

In fact, for this reasoning to work, the notion of ``tree-shaped'' need not
be as restrictive as what is shown in \Cref{fig:SQL-as-CQ}: for instance,
edges could be reversed. More generally, if the "query@@CQ" has "tree-width"%
\footnote{Tree-width is a graph-parameter that measure how far a graph is from a tree.
The notion can be extended to "relational structures".}
at most $k$, then we still get a polynomial-time evaluation algorithm---where the order
of the polynomial depends on $k$. In turns, it means that for every $k\in\Np$,
evaluating "conjunctive queries" that are semantically equivalent to
a "queries@CQ" of "tree-width" at most $k$ is "FPT".\footnote{In fact,
they can even be evaluated in polynomial time, but the argument is more involved
\cite{ChekuriRajaraman2000Containment}.}

Remarkably, this is \emph{exactly} the limit of tractability for these "queries@CQ":
Grohe showed that a class of "conjunctive queries" has "FPT" evaluation when parametrized in the 
size of the query "iff"
it has bounded ``"semantic tree-width"''---meaning that there exists $k\in\Np$ "st"
every "query@CQ" in the class is "semantically equivalent" to a "query@CQ" 
of "tree-width" at most $k$ \cite{Grohe2007ComplexityHomomorphism}.%
\footnote{Technical point: Actually for the equivalence to hold, the class of queries
needs to be recursively enumerable.}%
\footnote{The same equivalence holds for polynomial-time evaluation.}

\begin{known}
	"Conjunctive queries" of bounded semantic tree-width are exactly
	the classes of "conjunctive queries" with tractable evaluation.
\end{known}


\subsection{Adding Regular Path Predicates}

\begin{marginfigure}
	\centering
	\begin{tikzpicture}
		\node[vertex] at (0,2) (2) {};
		\node[vertex] at (0,1) (1) {};
		\node[vertex] at (0,0) (0) {};

		\draw[edge] (0) to (1);
		\draw[edge] (1) to (2);

		\node[font=\tiny, right=0em of 2] {\textsf{person}};
		\node[font=\tiny, right=0em of 1] {\textsf{child}};
		\node[font=\tiny, right=0em of 0] {\textsf{grand\_child}};

		\node[below= of 0] {output: \textsf{person}, \textsf{grand\_child}};
	\end{tikzpicture}
	\caption{
		\AP\label{fig:CQ-grandchild}
		A "conjunctive query" outputting all pairs of people with their grandchildren.
	}
\end{marginfigure}
Overall, the previous result prove that the theory of \emph{conjunctive queries} is well-understood.
However, even when considering other features from SQL, such as aggregate functions---\textsf{COUNT}, \textsf{SUM}, etc.---, the query language of "conjunctive queries" faces a big limitation:
it is \emph{intrinsically local}.
Consider two "structures" $\?A$ and $\?B$, and two elements $a$ and $a'$ of $A$.
For any homomorphism $f$ from $\?A$ to $\?B$, the "distance@@struct" from $f(a)$ to $f(a')$
in $\?B$ is at most the distance from $a$ to $a'$ in $\?A$.\footnote{This follows from
the definition of a "homomorphism", by using a trivial induction on the "distance@@struct".}
Now assume that $\?B$ is a "graph@@dir", whose vertices are humans,
and whose edges represent the `is a child of' relation.
For any $k\in\N$, it is easy to build a "conjunctive query" $\tup{\?A_k, \tup{a, a'}}$
outputting all pairs $\tup{b,b'}$ "st" $b'$ is a depth-$k$ descendant of $b$---see
\Cref{fig:CQ-grandchild} for $k=2$.
However, since "homomorphisms" contract "distances@@struct", there is no
"conjunctive query" $\tup{\?A_*, \tup{a, a'}}$ outputting all pairs $\tup{b,b'}$
"st" $b'$ is descendant, at \emph{any} depth, of $b$.
In other words, "conjunctive queries" are not closed under transitive closure.

More generally, human-centered data does not usually go well with
relational databases, as they are not designed to allow graph traversal.
To face this issue, "graph databases" have
been introduced: they can essentially be modelled
as "relational structures" whose relations are all binary. In other
words, they correspond to edge-labelled graphs, see \Cref{fig:example-wikidata}.%
\sidenote[][-7em]{If the reader is familiar with \emph{knowledge graphs},
everything we are saying about "graph databases" also applies
to knowledges graphs. While the two notions are distinct in practice,
their fundamental concept---and hence our theoretical modelling of them---is identical:
story data as a graph.}
To illustrate this point, we consider
"Wikidata", which is a "graph database" containing more than one hundred million
vertices, whose data is used amongst others on Wikipedia. 

\begin{figure}
	\lstinputlisting[language=mysparql]{fig/intro/exist.rq}
	\lstinputlisting[language=mysparql]{fig/intro/exist-bis.txt}
	\caption{
		\label{fig:sparql}
		A SPARQL query (above), together with a human-friendly translation (below).\\
	\href{https://query.wikidata.org/\#SELECT\%20DISTINCT\%20\%3Fwork\%20\%3FworkLabel\%20\%3FauthorLabel\%0AWHERE\%0A\%7B\%0A\%20\%20\%3Fwork\%09wdt\%3AP31\%2Fwdt\%3AP279\%2a\%20wd\%3AQ7725634\%3B\%0A\%20\%20\%20\%20\%20\%20\%20\%20rdfs\%3Alabel\%20\%3FworkLabel\%3B\%0A\%20\%20\%09\%09wdt\%3AP577\%20\%3Fdate\%3B\%0A\%20\%20\%20\%20\%20\%20\%20\%20wdt\%3AP50\%20\%3Fauthor.\%0A\%20\%20\%3Fauthor\%20rdfs\%3Alabel\%20\%3FauthorLabel.\%0A\%20\%20FILTER\%28LANG\%28\%3FworkLabel\%29\%20\%3D\%20\%22fr\%22\%20\%26\%26\%20LANG\%28\%3FauthorLabel\%29\%20\%3D\%20\%22fr\%22\%29.\%0A\%20\%20FILTER\%28CONTAINS\%28\%3FworkLabel\%2C\%20\%22exist\%22\%29\%29.\%0A\%20\%20FILTER\%28YEAR\%28\%3Fdate\%29\%20\%3C\%3D\%201990\%29.\%0A\%7D\%0ALIMIT\%207}{[\raisebox{-.4ex}{\HandRight} Run the query.]}
	}
\end{figure}
We would like to obtain all literary works published before 1990 and whose
French title contains the string ``exist''.
This can be done using the SPARQL query language, which is roughly the equivalent
of SQL for knowledge bases: the query is represented in \Cref{fig:sparql}.
The central notion in knowledge graphs and SPARQL is the notion of triplets:
\lstinline[language=SQL]{x R y.} refers to an edge of the \textsf{R}-relation
going from \textsf{x} to \textsf{y}.
Then \lstinline[language=SQL]{x R y; S z.} is an abbreviation for
the conjunction \lstinline[language=SQL]{x R y. x S z.}
Hence, the central part (ll.~4--8) of the SPARQL query of \Cref{fig:sparql}
should be understood as follows:
we are looking for variables \textsf{?work}, \textsf{?typeOfWork} (implicit),
\textsf{?workLabel}, \textsf{?date} and \textsf{?authorLabel} "st":
\begin{itemize}
	\item there is a path from \textsf{?work} to \textsf{?typeOfWork}
		obtained by taking an edge `is instance of', and then an arbitrary number
		of edges of type `is subclass of',
	\item \textsf{?typeOfWork} should exactly correspond to the vertex called `type of work',
	\item \textsf{?work} and \textsf{?author} have label \textsf{?workLabel} and \textsf{?authorLabel}, respectively, 
	\item \textsf{?work} was published on \textsf{?date}, and
	\item \textsf{?work} was authored by \textsf{?author}.
\end{itemize}
The key feature of query languages for "graph databases" is illustrated
by the \lstinline[language=mysparql]{wdt:P31/wdt:P279*} instruction:
this expression does not refer to a single edge of the knowledge graph,
but rather to a regular expression formed from these edges.
These regular expressions are in fact precisely what allows for easy graph traversal!
An example of a match of this expression is provided
in red in \Cref{fig:example-wikidata}.
The output of the query of \Cref{fig:sparql}
is provided in \Cref{tab:output-sparql}.

\begin{figure}
	\centering
	\begin{tikzpicture}
		\node[vertex] at (0,-.5) (stats) {};
		\node[vertex] at (-1.5,-1.5) (FF) {};
		\node[vertex] at (0,-2.1) (1892) {};
		\node[vertex] at (1.5,-1.5) (French) {};
		\node[vertex] at (0,1) (bioDic) {};
		\node[vertex] at (-1.5,2) (bio) {};
		\node[vertex] at (0,2) (dico) {};
		\node[vertex] at (1.5,2) (cata) {};
		\node[vertex] at (.5,3) (kl) {};
		\node[vertex] at (-.5,3) (ref) {};
		\node[vertex] at (-1.5,3) (bioW) {};
		\node[vertex] at (-1.5,4) (nonFic) {};
		\node[vertex] at (1.5,3) (lit) {};

		\draw[edge] (stats) to node[fill=white, midway, font=\tiny] {author} (FF);
		\draw[edge] (stats) to node[fill=white, midway, font=\tiny] {date} (1892);
		\draw[edge] (stats) to node[fill=white, midway, font=\tiny] {language} (French);
		\draw[edge, draw=c0] (stats) to node[fill=white, midway, font=\tiny, text=c0] {instance of} (bioDic);

		\draw[edge, bend left, dotted] (bioDic) to (bio);
		\draw[edge, bend right, dotted] (bioDic) to (cata);
		\draw[edge, dotted, draw=c0] (bioDic) to (dico);

		\draw[edge, dotted] (bio) to (bioW);
		\draw[edge, dotted] (bioW) to (nonFic);
		\draw[edge, dotted] (cata) to (ref);
		\draw[edge, dotted] (cata) to (kl);
		\draw[edge, dotted] (dico) to (kl);
		\draw[edge, dotted] (dico) to (ref);
		\draw[edge, dotted, draw=c0] (dico) to (lit);

		\node[right=1em of stats, font=\tiny, color=cDarkGrey, text width=3.5cm]
			{"Statistique des gens de lettres et des savants existant en France@@wd"};
		\node[below left=-.25em of FF, font=\tiny, color=cDarkGrey, text width=2cm, align=right]
			{"François-Fortuné Guyot de Fère@@wd"};
		\node[below=-.25em of 1892, font=\tiny, color=cDarkGrey]
			{1892};
		\node[below right=-.25em of French, font=\tiny, color=cDarkGrey]
			{"French@@wd"};
		\node[below left=-.5em and 1em of bioDic, font=\tiny, color=cDarkGrey, text width=1.5cm, align=right]
			{"biographical dictionary@@wd"};
		\node[left=-.25em of bio, font=\tiny, color=cDarkGrey]
			{"biography@@wd"};
		\node[below right=-.25em and -.25em of dico, font=\tiny, color=cDarkGrey]
			{"dictionary@@wd"};
		\node[right=-.25em of cata, font=\tiny, color=cDarkGrey]
			{"catalogue@@wd"};
		\node[above=.25em of kl, font=\tiny, color=cDarkGrey, text width=2cm, align=center]
			{"knowledge organization system@@wd"};
		\node[above=-.25em of ref, font=\tiny, color=cDarkGrey]
			{"reference work@@wd"};
		\node[left=-.25em of bioW, font=\tiny, color=cDarkGrey]
			{"biographical work@@wd"};
		\node[left=-.25em of nonFic, font=\tiny, color=cDarkGrey]
			{"non-fiction work@@wd"};
		\node[right=-.25em of lit, font=\tiny, color=cDarkGrey]
			{"literary work@@wd"};
	\end{tikzpicture}
	\caption{
		\AP\label{fig:example-wikidata}
		Part of the "Wikidata" graph database.
		Dotted arrows represent the relation `subclass of'.
		The red path matches the expression \lstinline[language=SQL]{wdt : P31 / wdt : P279 *}.
		For readability, labels are written next to the vertices
		rather than as separate vertices linked with a `has label' relation.
	}
\end{figure}
\begin{table}
	\centering
	{
		\footnotesize%
		\begin{tabular}{ll}
			\toprule
			workLabel & authorLabel \\ \midrule 
			Statistique des gens de lettres 
				& \multirow{2}{*}{François-Fortuné Guyot de Fère}\\
			et des savants existant en France
				& \\
			Le Chevalier inexistant
				& Italo Calvino\\
			L'existentialisme est un humanisme
				& Jean-Paul Sartre\\
			Ennui existentiel
				& Anton Tchekhov\\
			Les Ennuis de l'existence
				& Anton Tchekhov\\
			La tentation d'exister
				& Emil Cioran\\
			Inexistence
				& David Zindell \\ \bottomrule
		\end{tabular}
	}
	\caption{
		\AP\label{tab:output-sparql}
		Output of the SPARQL query of \Cref{fig:sparql}.}
\end{table}

\clearpage
We formalize the core features of SPARQL as "conjunctive regular path queries":
they consist of "conjunctive queries", except that their atoms are no
longer of the form $x \atom{\+R} y$ (for some binary relation $\+R \in \sigma$),
but can be more generally of the form
\[
	x \atom{L} y
	\quad\text{ for some "regular language" $L$ over $\sigma$}.
\]
For instance, ll.~1--8 of the SPARQL query of \Cref{fig:sparql}
can be modelled by the "conjunctive regular path query" of \Cref{fig:intro-SPARQL-as-CRPQ}:
notice the regular expression in red.
\begin{marginfigure}
	\centering
	\begin{tikzpicture}
		\node[vertex] at (0,0) (work) {};
		\node[vertex, above=3em of work] (litWork) {};
		\node[vertex, below left=2.2em and 3.5em of work] (workLabel) {};
		\node[vertex, below=2.9em of work] (date) {};
		\node[vertex, below right=2.2em and 3.5em of work] (author) {};
		\node[vertex, below=2.5em of author] (authorLabel) {};

		\draw[edge] (work) to node[fill=white, font=\tiny, text=c0] {instance $\cdot$ subclass${}^*$} (litWork);
		\draw[edge] (work) to node[fill=white, font=\tiny] {label} (workLabel);
		\draw[edge] (work) to node[fill=white, font=\tiny] {date} (date);
		\draw[edge] (work) to node[fill=white, font=\tiny] {author} (author);
		\draw[edge] (author) to node[fill=white, font=\tiny] {label} (authorLabel);

		\node[right=-.25em of work, font=\tiny, color=cDarkGrey]
			{work};
		\node[right=-.25em of litWork, font=\tiny, color=cDarkGrey]
			{typeOfWork};
		\node[below=-.25em of workLabel, font=\tiny, color=cDarkGrey]
			{workLabel};
		\node[below=-.25em of date, font=\tiny, color=cDarkGrey]
			{date};
		\node[right=-.25em of author, font=\tiny, color=cDarkGrey]
			{author};
		\node[below=-.25em of authorLabel, font=\tiny, color=cDarkGrey]
			{authorLabel};

		\node[below=4.5em of date] {output: \textsf{workLabel}, \textsf{authorLabel}};
	\end{tikzpicture}
	\caption{
		\AP\label{fig:intro-SPARQL-as-CRPQ}
		A "conjunctive regular path query" 
		modelling the core part of \Cref{fig:sparql}.
	}
\end{marginfigure}

\begin{known}
	"Graph databases"/knowledge graphs store information as edge-labelled graphs.
	To allow for graph traversal, we extend "conjunctive queries"
	to "conjunctive regular path queries" by adding regular expressions.
\end{known}


\subsection[Minimization and Structure of CRPQs]{Minimization and Structure of Conjunctive Regular Path Queries}

While "conjunctive regular path queries" share some enjoyable properties
of "conjunctive queries"---for instance the decidability of "semantical equivalence",
in contrast to "eg" "first-order logic"---its semantics is more complex:
graph-like phenomena ("homomorphisms") intertwine with "regular languages".
Not only does this lead to a complexity blow-up---"semantical equivalence" is "NP"-complete
for "conjunctive queries" but "ExpSpace"-complete for "conjunctive regular path queries"---,
it also breaks the nice theory of "cores".

\begin{marginfigure}[5em]
	\centering
	\includegraphics[width=\linewidth]{fig/intro/Gulliver.jpg}
	\caption{\href{https://www.musee-orsay.fr/fr/oeuvres/gulliver-lilliput-le-reveil-sur-la-plage-248152}{\emph{Gulliver à Lilliput : Le réveil sur la plage}}, by André Devambez.}
\end{marginfigure}
As a consequence, optimizing "conjunctive regular path queries" poses
a significant challenge to untwist graph properties from automata-theoretic ones.
This first part of this thesis is dedicated to this problem.
After exposing the basic theory of "conjunctive regular path queries"
in \Cref{ch:prelim-graph-databases}, we study
the "minimization problem@@CRPQ" in \Cref{ch:minimization-CRPQ}:
given such a "query@CRPQ" and $k\in\Np$, we can decide if it is equivalent to a "query@CRPQ"
of size at most $k$, and if so we can produce it.

\begin{contribution}
	Whether a "conjunctive regular path query" can be minimized is decidable,
	and minimization is effective.
\end{contribution}

We notice that, somewhat unexpectedly, there are some "conjunctive regular path queries"
that are minimal in the sense above, but that are equivalent to a \emph{finite union}---in the 
semantical sense---of strictly smaller "conjunctive regular path queries".%
\sidenote[][-5em]{This contrasts with the case of "conjunctive queries",
where the notion of "core" and the order-theoretic properties
of "relational structures" precisely prevents this phenomenon from appearing.
In other words, this phenomenon precisely emerges by interlacing
the graph structure and the automata of the "query@@CRPQ".}
We argue that measuring a union of such "queries@@CRPQ" by the maximal size
of a query in the union is a sensible thing to do---because the complexity
of evaluating such a union depends mostly on this parameter---, and prove
that given a "conjunctive regular path queries" and $k\in\N$, we can
decide if it is equivalent to a finite union of
"queries@CRPQ" which are all of size at most $k$.

\begin{contribution}
	Whether a "conjunctive regular path query" can be minimized
	as a union of strictly smaller "queries@@CRPQ" is decidable,
	and minimization is effective.
\end{contribution}

Both algorithms are essentially brute-force, and the main technical difficulty
lies in proving that there are finitely many candidates to test, which is not
trivial because we do not ask for any bound on the size of the "regular languages".%
\footnote{Again, this is motivated by the complexity of evaluating
a "conjunctive regular path queries", which mostly depends on how many "atoms"/edges it has,
and not so much on how complex these languages are.}
The idea behind the two algorithms are in fact surprisingly different:
\begin{itemize}
	\item in the first case---when union is not allowed---, we prove that
		if a "query@CRPQ" is equivalent to another one with few
		"atoms" (but potentially big languages), then it must be
		equivalent to a "query@CRPQ" with few "atoms" \emph{and} small languages.
		This property is proved by understanding the subtle interactions between
		languages and the graph structure;
	\item in the second case---when union is allowed---, we build a canonical
		finite union of "queries@CRPQ", corresponding to the
		"maximal under-approximation" by a finite union of small "queries@CRPQ": 
		it is the \emph{best} under-approximation---in the sense that
		it logically entails the "query@CRPQ"---and so, if the original
		"query@CRPQ" is equivalent to a finite union of small ones, then
		it must be equivalent to this "maximal under-approximation".
		The difficulty there lies in proving the existence of "maximal under-approximation",
		or rather that it can be expressed by a \emph{finite} union.
		This construction can essentially be seen as a
		smart brute-forcing, obtained by agglomerating all possible smaller "queries@CRPQ".
\end{itemize}

One reason we resolve to using brute-force algorithm is that it is
remarkably hard to understand when a "query@CRPQ" cannot be minimized.
The case of "conjunctive queries" is much simpler: if the "core" 
of the "query@CQ" has $k$ edges (resp. "tree-width" $k$),
then any "conjunctive query" "semantically equivalent" to it must use at least $k$ edges
(resp. have "tree-width" at least $k$).

Another of our contributions is to identify a sufficient condition
on a "query@CRPQ" so that \emph{any} "query@CRPQ" that is "semantically equivalent" to it
must contain a ``complex pattern''. The strength of this theorem lies in its
general applicability, as the notion of ``complex pattern''
is formalized as a ``"minor-closed" class
of graphs''---examples include the class of all graphs with at most $k$ "atoms",
or the class of all graphs of "tree-width" at most $k$.

\begin{contribution}
	We introduce the "semantical structure theorem", that provides
	a way to prove lower bounds on the number
	of "atoms", or "tree-width", or in fact any minor-closed property,
	that is necessary to express a "query@@sem".
\end{contribution}

This tool proves useful to show minimality of specific examples---"ie"
for proofs---, and to prove complexity lower bounds for our problem.
However, this only provides a sufficient condition, that is often not
necessary, it fails to provide a \emph{simple} algorithm to test minimality---hence our
brute-force algorithms.

\bigskip
Then, in \Cref{ch:semantic-tree-width-CRPQ}, we turn to the question
of "tree-width". Similarly to "conjunctive queries", finite unions of
"conjunctive regular path queries"
of bounded "tree-width" can be evaluated in polynomial time.
It begs the question of deciding when a "query@CRPQ" is actually equivalent
to a query of small "tree-width".

\begin{known}
	Barceló, Romero and Vardi \cite{BarceloRomeroVardi2016SemanticAcyclicity}
	devised an algorithm to test
	if a "conjunctive regular path queries" is equivalent to
	a finite union of ``acyclic''---meaning of "tree-width" 1---"queries@CRPQ".
\end{known}

The general question for "tree-width" $k$ is left open in their paper as the authors did not know how to extend their technique to this
more general setting. We extend their result, relying again on the notion
of "maximal under-approximation":%
\footnote{The paper of Barceló, Romero and Vardi
also relies on "maximal under-approximations", and this notion already existed
for "conjunctive queries".}
we prove the existence and computability of the "maximal under-approximation"
by finite unions of "queries@CRPQ" of "tree-width" of a given
"conjunctive regular path query".

\begin{contribution}
	Given $k\in \Np$ and a "conjunctive regular path queries",
	we can decide if the latter is "semantically equivalent"
	to a finite union of "queries@CRPQ" of "tree-width"
	(resp. "path-width") at most $k$.
\end{contribution}

The proof of existence of this "maximal under-approximation" is
much harder than in the case minimizing
the number of "atoms". It needs to deal with two kinds of information:
the structure of the query, "ie" its underlying graph, and its languages,
and so the proof precisely massages the query to
preserve information, at the same time, about the "tree decomposition"---serving as a
witness of the small "tree-width" of the "query@CRPQ"---and about the
semantics of the "query@CRPQ".

Amusingly, we originally thought that our proof was not able to
capture the case $k=1$ that was already handled, and that the constructions
of Barceló, Romero and Vardi and ours were orthogonal.
While writing the journal version of this paper---that was originally
published at ICDT '23---, we wanted to extend the results to "path-width",%
\sidenote[][-4em]{The main motivation behind this is that the evaluation of "queries@CRPQ"
of bounded "path-width" is not only polynomial but even "NL"!}
but part of our construction broke. Introducing the technical tool
to fix it%
\sidenote[][]{See the notions of "contractions" and "contracted path-width".}
actually leads to a unified solution, that handles both
the case of "tree-width" $k$ (including $k=1$) and "path-width".%
\sidenote[][]{The order of presentation of these results
does not follow the timeline of their discovery: our work on "semantic tree-width"
was done in 2022--23 and published at ICDT '23, while the one on minimization
was done in 2024--25 and published at PODS '25.}

Lastly, all these algorithms rely on testing the equivalence
of "conjunctive regular path queries", which is "ExpSpace"-complete.
It leads to resource-hungry algorithms---although it has to be noted that it is
worth running exponential algorithms on smallish "queries@CRPQ" in order to optimize
their evaluation on huge databases!---which leads to a natural quest
for identifying subclasses of "queries@CRPQ" that admit more efficient algorithms.

As witnessed by the example of \Cref{fig:sparql}, the regular expressions
used in practice are often much simpler than the one required
to obtain the "ExpSpace"-hardness of the equivalence problem.
Fortunately, "conjunctive regular path queries" over "simple regular expression",
where the "regular languages" allowed are restricted to be concatenations of
edge labels and reflexive-transitive closure ("aka" Kleene star) of
edge labels, were already known to have a more efficient
algorithm for testing "semantical equivalence".

\begin{contribution}
	We prove that the problem of minimizing the number of "atoms" (resp. "tree-width")
	of "conjunctive regular path queries" over "simple regular expressions"
	lies in the polynomial hierarchy.
\end{contribution}

A consequence of our work on "tree-width" is that,
given a "conjunctive regular path query" that is promised to be equivalent
to a "query@CRPQ" of "tree-width" $k$, we can first compute said
equivalent query of small "tree-width" by using our algorithm,
and then evaluate it on any database. This proves that the evaluation
problem for "conjunctive regular path queries" of bounded "semantic tree-width"
is "FPT" when parametrized by the size of the "query@@sem".%
\footnote{This result was in fact already known---but proven differently, with
an incomparable complexity (better preprocessing but worst polynomial exponent)---by
Romero, Barceló and Vardi \cite{RomeroBarceloVardi2017Homomorphism}.}

Whether the converse holds remains a mystery: many attempts
have been tried to extend Grohe's proof for "conjunctive queries" to this setting,
but all failed, precisely because of the difficulty posed
by the intertwining of the graph structure and the automata.
We conclude this part of the thesis by a discussion of this problem,
as well as whimsical ideas related to "conjunctive regular path queries"
in \Cref{ch:conclu-database}.

\begin{openproblemintro}
	Characterize the classes of "CRPQs" with "FPT" evaluation
	when parametrized by the size of the "query@CRPQ".
\end{openproblemintro}

To summarize, the $\textsf{query} \homto^? \textsf{data}$ formulation of the "homomorphism problem" provides a robust foundation for classical database theory. The first part of this thesis extends this framework to a richer context: graph databases and queries extended with regular path predicates.